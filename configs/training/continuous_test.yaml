exp_name: continuous-training-test
wandb_project: guided-diffusion-protein

data:
  name: trpB
  dataset_path: data/trpB_processed.csv
  splits: [train, validation]

model:
  _target_: models.model.continuous_diffusion.GaussianDiffusion

  # Add a "model_config" just to keep shape consistent:
  model_config:
    name: continuous_diffusion  # used for your check
    _lambda: 0
    mask: uniform
    device: cuda:1

  network:
    _target_: models.model.continuous_diffusion.GaussianDiffusionTransformer
    in_channels: 64            # choose an embedding dimension as needed
    vocab_size: 20             # length of protein alphabet, e.g. len(MSA_ALPHABET)
    dropout: 0.1               # dropout rate
    bert_config_name: bert-base-uncased
    target_channels: 0         # if you don't need regression targets
    discr_stop_grad: True

  noise_schedule:
    _target_: models.collaters.GaussianDiffusionSchedule
    timesteps: 500
    noise_schedule: cosine    # or linear, quadratic, etc. as supported
    noise_scale: 1.0

  optimizer:
    _target_: torch.optim.AdamW
    lr: 1e-4

  lr_scheduler:
    _target_: transformers.get_linear_schedule_with_warmup
    num_warmup_steps: 10
    num_training_steps: 10  # adjust based on training length/epochs

tokenizer:
  _target_: evodiff.utils.Tokenizer

train:
  seed: 42
  gradient_clip: 10.0
  min_epochs: 100
  max_epochs: 100
  early_stop_patience: 0
  batch_size: 1
  val_interval: 1
  log_interval: 10
  workers: 4
  ngpu: 1
